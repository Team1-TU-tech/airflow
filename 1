from datetime import datetime
from datetime import timedelta
from airflow import DAG
from airflow.operators.python import PythonVirtualenvOperator, PythonOperator, EmptyOperator
import os
import io
import requests
from kafka import KafkaConsumer, KafkaProducer
from json import loads, dumps
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

def producer_to_kafka():
    producer = KafkaProducer(
            bootstrap_servers=['kafka:9092'],
            value_serializer=lambda x: dumps(x).encode('utf-8')
        )
    
    from interpark.raw_ticket_page import extract_ticket_html
    get_data = extract_ticket_html()
    for data in get_data:
        try:
            # 데이터를 문자열로 가정하고 io.StringIO로 처리
            soup = data["data"]  # 크롤링 데이터의 HTML 내용

            # BeautifulSoup 객체를 HTML 문자열로 변환
            if hasattr(soup, "prettify"):
                html_content = soup.prettify()  # 예쁘게 정리된 HTML
            else:
                html_content = str(soup)  # 일반 문자열로 변환

            if not html_content.strip():  # HTML 데이터가 비어 있는지 확인
                raise ValueError("HTML 데이터가 비어 있습니다.")

            file_obj = io.BytesIO(html_content.encode('utf-8'))
            
            # kafka 메세지 생
            message  = {'title': f'{data["num"]}_{data["ticket_num"]}',
                        'save_path': f's3://t1-tu-data/interpark/{data["num"]}_{data["ticket_num"]}.html',
                        'contents' : file_obj.getvalue()
                        }   
        except:
            print("데이터가 없습니다.")
    
    if messgae:
        producer.send('raw_interpark_data', value=message)
        producer.flush()
        print(f"카프카로 전송 완료:{message}")


def kafka_to_s3():
        consumer = KafkaConsumer(
                'raw_interpark_data',
                bootstrap_servers='kafka:9092',
                auto_offset_reset="earliest",
                group_id='interpark_s3',
                value_deserializer=lambda x: loads(x.decode('utf-8')),
                consumer_timeout_ms=1000,
                )
        print("kafka 연결 성공")
    
       # 컨슈머 연결되면 s3로 전송
        while True:
            msg = consumer.poll(timeout=1.0) # 1초 대기
            
            # 메세지 없으면 기다림
            if msg is None:
                continue 
            elif msg.error():
                if msg.error().code() == KafkaError._PARTITON_EOF:
                    print(f"End of partition reached: {msg.topic()} [{msg.partition}]")
                else:
                    print(f"Consumer error: {msg.error()}")
            # 메세지 있으면 s3로 전송
            else: 
                for record in msg.get('raw_interpark_data',[]): # raw_interpark_data topic에서 메세지 순회
                    hook = S3Hook(aws_conn_id='interpark') #s3 연결
                    data = record.value # kafka에서 message 불러오기
                    key = f'interpark/{test_data["num"]}_{data["ticket_num"]}.html'
                    file_obj = io.BytesIO(data['data'])
                    bucket_name = 't1-tu-data'
                    hook = S3Hook(awdds_conn_id='interpark')
                    try:
                        hook.get_conn().put_object(
                        Bucket=bucket_name,
                        Key=key,
                        Body=file_obj
                        )       
                        print(f"S3에 업로드 완료: {bucket_name}/{key}")
                        break

                    except Exception as e:
                        print(f"S3 업로드 실패: {e}")
                        continue

def success_noti():
    KEY = os.getenv("LINE_TOKEN")
    url = "https://notify-api.line.me/api/notify"
    data = {"message":"airflow 작업 완료👍"}
    headers={"Authorization":f"Bearer {KEY}"}
    response = requests.post(url, data, headers=headers)
    return True

def fail_noti():
    KEY = os.getenv("LINE_TOKEN")
    url = "https://notify-api.line.me/api/notify"
    data = {"message":"airflow 작업 🔥실패🔥"}
    headers={"Authorization":f"Bearer {KEY}"}
    response = requests.post(url, data, headers=headers)
    return True

with DAG(
'kafka_to_S3',
default_args={
'depends_on_past': False,
'email_on_failure': False,
'email_on_retry': False,
'retries': 3,
'retry_delay':timedelta(minutes=3),
},
description='interpark DAG',
start_date=datetime(2024, 11, 21),
schedule_interval='@daily',
catchup=False,
tags=['interpark','kafka','s3']
) as dag:

    start = EmptyOperator(
            task_id='start'
            )

    end = EmptyOperator(
            task_id='end'
            )

    producer_to_kafka = PythonVirtualenvOperator(
            task_id='producer.to.kafka',
            python_callable=producer_to_kafka,
            requirements=[
                "git+https://github.com/hahahellooo/interpark.git@0.4/s3"
                ],
            system_site_packages=True,
            )
    
    kafka_to_s3 = PythonOperator(
            task_id='kafka.to.s3',
            python_callable=kafka_to_s3
            )
    
    success_noti = PythonOperator(
            task_id='success.noti',
            python_callable=success_noti,
            trigger_rule="all_success"
            )

    fail_noti = PythonOperator(
            task_id='fail.noti',
            python_callable=fail_noti,
            trigger_rule="one_fail"
            )

    start >> producer_to_kafka >> kafka_to_s3 >> success_noti, fail_noti >> end
