from datetime import datetime
from datetime import timedelta
from airflow import DAG
from airflow.operators.python import PythonVirtualenvOperator, PythonOperator, EmptyOperator
import os
import io
import requests
from kafka import KafkaConsumer, KafkaProducer
from json import loads, dumps
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

def producer_to_kafka():
    producer = KafkaProducer(
            bootstrap_servers=['kafka:9092'],
            value_serializer=lambda x: dumps(x).encode('utf-8')
        )
    
    from interpark.raw_ticket_page import extract_ticket_html
    get_data = extract_ticket_html()
    for data in get_data:
        try:
            # ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ê°€ì •í•˜ê³  io.StringIOë¡œ ì²˜ë¦¬
            soup = data["data"]  # í¬ë¡¤ë§ ë°ì´í„°ì˜ HTML ë‚´ìš©

            # BeautifulSoup ê°ì²´ë¥¼ HTML ë¬¸ìì—´ë¡œ ë³€í™˜
            if hasattr(soup, "prettify"):
                html_content = soup.prettify()  # ì˜ˆì˜ê²Œ ì •ë¦¬ëœ HTML
            else:
                html_content = str(soup)  # ì¼ë°˜ ë¬¸ìì—´ë¡œ ë³€í™˜

            if not html_content.strip():  # HTML ë°ì´í„°ê°€ ë¹„ì–´ ìˆëŠ”ì§€ í™•ì¸
                raise ValueError("HTML ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

            file_obj = io.BytesIO(html_content.encode('utf-8'))
            
            # kafka ë©”ì„¸ì§€ ìƒ
            message  = {'title': f'{data["num"]}_{data["ticket_num"]}',
                        'save_path': f's3://t1-tu-data/interpark/{data["num"]}_{data["ticket_num"]}.html',
                        'contents' : file_obj.getvalue()
                        }   
        except:
            print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    if messgae:
        producer.send('raw_interpark_data', value=message)
        producer.flush()
        print(f"ì¹´í”„ì¹´ë¡œ ì „ì†¡ ì™„ë£Œ:{message}")


def kafka_to_s3():
        consumer = KafkaConsumer(
                'raw_interpark_data',
                bootstrap_servers='kafka:9092',
                auto_offset_reset="earliest",
                group_id='interpark_s3',
                value_deserializer=lambda x: loads(x.decode('utf-8')),
                consumer_timeout_ms=1000,
                )
        print("kafka ì—°ê²° ì„±ê³µ")
    
       # ì»¨ìŠˆë¨¸ ì—°ê²°ë˜ë©´ s3ë¡œ ì „ì†¡
        while True:
            msg = consumer.poll(timeout=1.0) # 1ì´ˆ ëŒ€ê¸°
            
            # ë©”ì„¸ì§€ ì—†ìœ¼ë©´ ê¸°ë‹¤ë¦¼
            if msg is None:
                continue 
            elif msg.error():
                if msg.error().code() == KafkaError._PARTITON_EOF:
                    print(f"End of partition reached: {msg.topic()} [{msg.partition}]")
                else:
                    print(f"Consumer error: {msg.error()}")
            # ë©”ì„¸ì§€ ìˆìœ¼ë©´ s3ë¡œ ì „ì†¡
            else: 
                for record in msg.get('raw_interpark_data',[]): # raw_interpark_data topicì—ì„œ ë©”ì„¸ì§€ ìˆœíšŒ
                    hook = S3Hook(aws_conn_id='interpark') #s3 ì—°ê²°
                    data = record.value # kafkaì—ì„œ message ë¶ˆëŸ¬ì˜¤ê¸°
                    key = f'interpark/{test_data["num"]}_{data["ticket_num"]}.html'
                    file_obj = io.BytesIO(data['data'])
                    bucket_name = 't1-tu-data'
                    hook = S3Hook(awdds_conn_id='interpark')
                    try:
                        hook.get_conn().put_object(
                        Bucket=bucket_name,
                        Key=key,
                        Body=file_obj
                        )       
                        print(f"S3ì— ì—…ë¡œë“œ ì™„ë£Œ: {bucket_name}/{key}")
                        break

                    except Exception as e:
                        print(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue

def success_noti():
    KEY = os.getenv("LINE_TOKEN")
    url = "https://notify-api.line.me/api/notify"
    data = {"message":"airflow ì‘ì—… ì™„ë£ŒğŸ‘"}
    headers={"Authorization":f"Bearer {KEY}"}
    response = requests.post(url, data, headers=headers)
    return True

def fail_noti():
    KEY = os.getenv("LINE_TOKEN")
    url = "https://notify-api.line.me/api/notify"
    data = {"message":"airflow ì‘ì—… ğŸ”¥ì‹¤íŒ¨ğŸ”¥"}
    headers={"Authorization":f"Bearer {KEY}"}
    response = requests.post(url, data, headers=headers)
    return True

with DAG(
'kafka_to_S3',
default_args={
'depends_on_past': False,
'email_on_failure': False,
'email_on_retry': False,
'retries': 3,
'retry_delay':timedelta(minutes=3),
},
description='interpark DAG',
start_date=datetime(2024, 11, 21),
schedule_interval='@daily',
catchup=False,
tags=['interpark','kafka','s3']
) as dag:

    start = EmptyOperator(
            task_id='start'
            )

    end = EmptyOperator(
            task_id='end'
            )

    producer_to_kafka = PythonVirtualenvOperator(
            task_id='producer.to.kafka',
            python_callable=producer_to_kafka,
            requirements=[
                "git+https://github.com/hahahellooo/interpark.git@0.4/s3"
                ],
            system_site_packages=True,
            )
    
    kafka_to_s3 = PythonOperator(
            task_id='kafka.to.s3',
            python_callable=kafka_to_s3
            )
    
    success_noti = PythonOperator(
            task_id='success.noti',
            python_callable=success_noti,
            trigger_rule="all_success"
            )

    fail_noti = PythonOperator(
            task_id='fail.noti',
            python_callable=fail_noti,
            trigger_rule="one_fail"
            )

    start >> producer_to_kafka >> kafka_to_s3 >> success_noti, fail_noti >> end
