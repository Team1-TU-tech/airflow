from pymongo import MongoClient
from airflow import DAG
from airflow.operators.python import PythonVirtualenvOperator, PythonOperator
import time
from datetime import timedelta, datetime
from airflow.operators.empty import EmptyOperator
from kafka import KafkaConsumer
from pymongo import MongoClient
import requests
import json
import os
from dotenv import load_dotenv

load_dotenv()  # .env íŒŒì¼ì—ì„œ ë³€ìˆ˜ ë¡œë“œ

mongo_uri = os.getenv("MONGO_URI")

def s3_to_kafka():
    from crawling.read_s3_parsing import html_parsing, extract_data, convert_to_datetime_format, get_region
    message = html_parsing(52879)
    print("ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")

def consumer_to_mongo():
    attempt = 0
    connected = False
    retry_count = 3

    try:
        client = MongoClient(mongo_uri)  # MongoDB ì—°ê²°
        db = client['test']  # ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
        collection = db['test']  # ì»¬ë ‰ì…˜ ì´ë¦„
        print(mongo_uri)
        print(client)
        print("MongoDB ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # kafka ì—°ê²°
    while attempt < retry_count and not connected:
        try:
            consumer = KafkaConsumer(
                'interpark_data',
                bootstrap_servers= ['kafka1:9093','kafka2:9094','kafka3:9095'],
                auto_offset_reset="earliest",
                group_id='interpark_mongo',
                value_deserializer=lambda x: json.loads(x.decode('utf-8')),
                consumer_timeout_ms=5000,
            )

            print("kafka ì—°ê²° ì„±ê³µ")
            connected = True

            # ì»¨ìŠˆë¨¸ ì—°ê²°ë˜ë©´ s3ë¡œ ì „ì†¡
            empty_count = 0  # ë©”ì‹œì§€ê°€ ì—†ì„ ë•Œ ì¹´ìš´íŠ¸í•  ë³€ìˆ˜
            while True:
                msg = consumer.poll(timeout_ms=3000)
                # ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ê¸°ë‹¤ë¦¼
                if not msg:
                    empty_count += 1
                    print(f"{empty_count}: ë©”ì„¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    # 3ë²ˆ ì—°ì†ìœ¼ë¡œ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    if empty_count >= 3:
                        print("ë©”ì‹œì§€ê°€ 3ë²ˆ ì—°ì†ìœ¼ë¡œ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    continue

                else:
                    empty_count = 0  # ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
                    for message in msg.values():
                        for data in message:
                            try:
                                # Kafkaì—ì„œ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜¤ê¸°
                                data = data.value
                                print(f"Kafkaì—ì„œ ë°›ì€ ë°ì´í„°: {data}")

                                collection.insert_one(data)
                                print(f"mongodbì— ë°ì´í„° ì €ì¥ ì„±ê³µ")
                            except Exception as e:
                                print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        except Exception as e:
            print(f"kafka ì—°ê²° ì‹¤íŒ¨: {e}")
            attempt += 1
            print(f"{attempt}/{retry_count} ë²ˆì§¸ ì‹œë„ ì¤‘...")
            time.sleep(5)  # 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
   
    # Kafka Consumer ì¢…ë£Œ
    print("Consumer ì¢…ë£Œ")
    consumer.close()

def success_noti():
    url = "https://notify-api.line.me/api/notify"
    data = {"message":"MongoDBì— ë°ì´í„° ì €ì¥ ì™„ë£Œ ğŸ‘"}
    headers={"Authorization": 'Bearer UuAPZM7msPnFaJt5wXTUx34JqYKO7n3AUlLq4b3eyZ4'}
    response = requests.post(url, data, headers=headers)
    print("#"*35)
    print("airflow ì‘ì—…ì™„ë£Œ")
    print("#"*35)
    return True

def fail_noti():
    url = "https://notify-api.line.me/api/notify"
    data = {"message":"MongoDBì— ë°ì´í„° ì €ì¥ ğŸ”¥ì‹¤íŒ¨ğŸ”¥"}
    headers={"Authorization": 'Bearer UuAPZM7msPnFaJt5wXTUx34JqYKO7n3AUlLq4b3eyZ4'}
    response = requests.post(url, data, headers=headers)
    print("#"*35)
    print("airflow ì‘ì—…ì‹¤íŒ¨")
    print("#"*35)
    return True

with DAG(
'interpark_to_mongo',
default_args={
'email_on_failure': False,
'email_on_retry': False,
'execution_timeout': timedelta(minutes=120),
'retries': 3,
'retry_delay':timedelta(minutes=3),
},
description='interpark data DAG',
start_date=datetime(2024, 11, 30),
schedule_interval='@daily',
catchup=False,
tags=['interpark','kafka','s3', 'mongoDB']
) as dag:

    start = EmptyOperator(
            task_id='start'
            )

    end = EmptyOperator(
            task_id='end',
            trigger_rule ="one_success"
            )

    s3_to_kafka = PythonVirtualenvOperator(
            task_id='s3.to.kafka',
            python_callable=s3_to_kafka,
            requirements=[
                "git+https://github.com/Team1-TU-tech/crawling.git@interpark"
                ],
            system_site_packages=True
            )

    kafka_to_mongo = PythonOperator(
            task_id='kafka.to.mongo',
            python_callable=consumer_to_mongo
            )

    success_noti = PythonOperator(
            task_id='success.noti',
            python_callable=success_noti
            )

    fail_noti = PythonOperator(
            task_id='fail.noti',
            python_callable=fail_noti,
            trigger_rule="one_failed"
            )

    start >> s3_to_kafka >> kafka_to_mongo
    kafka_to_mongo >> [success_noti, fail_noti] >> end
