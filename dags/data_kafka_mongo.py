from pymongo import MongoClient
from airflow import DAG
from airflow.operators.python import PythonVirtualenvOperator, PythonOperator
import time
from datetime import timedelta, datetime
from airflow.operators.empty import EmptyOperator
from kafka import KafkaConsumer
from pymongo import MongoClient
import requests
import json

def s3_to_kafka():
    from interpark.read_s3_parsing import html_parsing, extract_data, convert_to_datetime_format
    from interpark.region import get_location
    data = html_parsing()
    print("ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")

    from kafka import KafkaProducer
    import json
    producer = KafkaProducer(
            bootstrap_servers = ['kafka1:9093','kafka2:9094', 'kafka3:9095'],
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
            )
    topic = 'interpark_data'
    print("==========================================")
    try:
        # ticket_data í˜•ì‹ì— ë§ê²Œ ë©”ì‹œì§€ ì‘ì„±
        message = {
                "title": data.get("title"),
                "category": data.get("category"),
                "location": data.get("location"),
                "region": data.get("region"),
                "price": data.get("price"),
                "start_date": data.get("start_date"),
                "end_date": data.get("end_date"),
                "show_time": data.get("show_time"),
                "running_time": data.get("running_time"),
                "casting": data.get("casting"),
                "rating": data.get("rating"),
                "description": data.get("description"),
                "poster_url": data.get("poster_url"),
                "open_date": data.get("open_date"),
                "pre_open_date": data.get("pre_open_date"),
                "artist": data.get("artist"),
                "hosts": data.get("hosts")
                }
            
        producer.send(topic, message)
        print("ì¹´í”„ì¹´ë¡œ ì „ì†¡ ì™„ë£Œ")
    except ValueError as ve:
        print(f"ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {ve}")
    except Exception as e:
        print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ëª¨ë“  ë°ì´í„° ì „ì†¡ í›„ flush
    producer.flush()

def consumer_to_mongo():
    attempt = 0
    connected = False
    retry_count = 3

    try:
        client = MongoClient("mongodb+srv://hahahello777:akXSTBrO5Q5OkWb3@cluster0.5vlv3.mongodb.net/test?retryWrites=true&w=majority&appName=Cluster0")  # MongoDB ì—°ê²°
        db = client['test']  # ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
        collection = db['test']  # ì»¬ë ‰ì…˜ ì´ë¦„
        print("MongoDB ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # kafka ì—°ê²°
    while attempt < retry_count and not connected:
        try:
            consumer = KafkaConsumer(
                'interpark_data',
                bootstrap_servers= ['kafka1:9093','kafka2:9094','kafka3:9095'],
                auto_offset_reset="earliest",
                group_id='interpark_mongo',
                value_deserializer=lambda x: json.loads(x.decode('utf-8')),
                consumer_timeout_ms=3000,
            )

            print("kafka ì—°ê²° ì„±ê³µ")
            connected = True

            # ì»¨ìŠˆë¨¸ ì—°ê²°ë˜ë©´ s3ë¡œ ì „ì†¡
            empty_count = 0  # ë©”ì‹œì§€ê°€ ì—†ì„ ë•Œ ì¹´ìš´íŠ¸í•  ë³€ìˆ˜
            while True:
                msg = consumer.poll(timeout_ms=1000)
                # ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ê¸°ë‹¤ë¦¼
                if not msg:
                    empty_count += 1
                    print(f"{empty_count}: ë©”ì„¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    # 3ë²ˆ ì—°ì†ìœ¼ë¡œ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    if empty_count >= 3:
                        print("ë©”ì‹œì§€ê°€ 3ë²ˆ ì—°ì†ìœ¼ë¡œ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    continue

                else:
                    empty_count = 0  # ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
                    for message in msg.values():
                        for data in message:
                            try:
                                # Kafkaì—ì„œ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜¤ê¸°
                                data = data.value
                                print(f"Kiafkaì—ì„œ ë°›ì€ ë°ì´í„°: {data}")

                                collection.insert_one(data)
                                print(f"mongodbì— ë°ì´í„° ì €ì¥ ì„±ê³µ")
                            except Exception as e:
                                print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        except Exception as e:
            print(f"kafka ì—°ê²° ì‹¤íŒ¨: {e}")
            attempt += 1
            print(f"{attempt}/{retry_count} ë²ˆì§¸ ì‹œë„ ì¤‘...")
            time.sleep(5)  # 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
   
    # Kafka Consumer ì¢…ë£Œ
    print("Consumer ì¢…ë£Œ")
    consumer.close()

def success_noti():
    url = "https://notify-api.line.me/api/notify"
    data = {"message":"MongoDBì— ë°ì´í„° ì €ì¥ ì™„ë£Œ ğŸ‘"}
    headers={"Authorization": 'Bearer UuAPZM7msPnFaJt5wXTUx34JqYKO7n3AUlLq4b3eyZ4'}
    response = requests.post(url, data, headers=headers)
    print("#"*35)
    print("airflow ì‘ì—…ì™„ë£Œ")
    print("#"*35)
    return True

def fail_noti():
    url = "https://notify-api.line.me/api/notify"
    data = {"message":"MongoDBì— ë°ì´í„° ì €ì¥ ğŸ”¥ì‹¤íŒ¨ğŸ”¥"}
    headers={"Authorization": 'Bearer UuAPZM7msPnFaJt5wXTUx34JqYKO7n3AUlLq4b3eyZ4'}
    response = requests.post(url, data, headers=headers)
    print("#"*35)
    print("airflow ì‘ì—…ì‹¤íŒ¨")
    print("#"*35)
    return True

with DAG(
's3_to_kafka_to_mongo',
default_args={
'email_on_failure': False,
'email_on_retry': False,
'execution_timeout': timedelta(minutes=10),
'retries': 3,
'retry_delay':timedelta(minutes=3),
},
description='interpark data DAG',
start_date=datetime(2024, 11, 30),
schedule_interval='@daily',
catchup=False,
tags=['interpark','kafka','s3', 'mongoDB']
) as dag:

    start = EmptyOperator(
            task_id='start'
            )

    end = EmptyOperator(
            task_id='end',
            trigger_rule ="one_success"
            )

    s3_to_kafka = PythonVirtualenvOperator(
            task_id='s3.to.kafka',
            python_callable=s3_to_kafka,
            requirements=[
                "git+https://github.com/hahahellooo/interpark.git@0.5/mongo"
                ],
            system_site_packages=True
            )

    kafka_to_mongo = PythonOperator(
            task_id='kafka.to.mongo',
            python_callable=consumer_to_mongo
            )

    success_noti = PythonOperator(
            task_id='success.noti',
            python_callable=success_noti
            )

    fail_noti = PythonOperator(
            task_id='fail.noti',
            python_callable=fail_noti,
            trigger_rule="one_failed"
            )

    start >> s3_to_kafka >> kafka_to_mongo
    kafka_to_mongo >> [success_noti, fail_noti] >> end
