from airflow.utils.trigger_rule import TriggerRule
from datetime import datetime
from datetime import timedelta
from airflow import DAG
from airflow.operators.python import PythonVirtualenvOperator, PythonOperator
from airflow.operators.empty import EmptyOperator
import io, time
from kafka import KafkaConsumer
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import requests
import json
import base64

def producer_to_kafka():
    from interpark.raw_ticket_page import extract_ticket_html

    get_data = extract_ticket_html()
    print("ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")
    
    from kafka import KafkaProducer
    import json
    import io
    import base64
    producer = KafkaProducer(
            bootstrap_servers= ['kafka1:9092','kafka2:9093','kafka3:9094'],
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
    topic = 'raw_interpark_data'
    for data in get_data:
        try:
            # ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ê°€ì •í•˜ê³  io.StringIOë¡œ ì²˜ë¦¬
            soup = data["data"]  # í¬ë¡¤ë§ ë°ì´í„°ì˜ HTML ë‚´ìš©

            # BeautifulSoup ê°ì²´ë¥¼ HTML ë¬¸ìì—´ë¡œ ë³€í™˜
            if hasattr(soup, "prettify"):
                html_content = soup.prettify()  # ì˜ˆì˜ê²Œ ì •ë¦¬ëœ HTML
            else:
                html_content = str(soup)  # ì¼ë°˜ ë¬¸ìì—´ë¡œ ë³€í™˜

            if not html_content.strip():  # HTML ë°ì´í„°ê°€ ë¹„ì–´ ìˆëŠ”ì§€ í™•ì¸
                raise ValueError("HTML ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            
            # HTMLì„ ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”©í•˜ì—¬ íŒŒì¼ ê°ì²´ë¡œ ì €ì¥
            file_obj = io.BytesIO(html_content.encode('utf-8'))
            # ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜ 
            encoded_content = base64.b64encode(file_obj.getvalue()).decode('utf-8')
            # kafka ë©”ì„¸ì§€ ìƒì„±
            message  = {'title': f'kafka_{data["num"]}_{data["ticket_num"]}.html',
                        'save_path': f'interpark/kafka_{data["num"]}_{data["ticket_num"]}.html',
                        'contents' : encoded_content
                        }  
            
            producer.send(topic, value=message)  
            print(f"ì¹´í”„ì¹´ë¡œ ì „ì†¡ ì™„ë£Œ:{message}")

        except ValueError as ve:
            print(f"ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {ve}")
        except Exception as e:
            print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ëª¨ë“  ë°ì´í„° ì „ì†¡ í›„ flush
    producer.flush()


def kafka_to_s3():
    import json
    retry_count = 3
    attempt = 0
    connected = False

    # kafka ì—°ê²°
    while attempt < retry_count and not connected:
        try:
            consumer = KafkaConsumer(
                'raw_interpark_data',
                bootstrap_servers= ['kafka1:9092','kafka2:9093','kafka3:9094'],
                auto_offset_reset="earliest",
                group_id='interpark_s3',
                value_deserializer=lambda x: json.loads(x.decode('utf-8')),
                consumer_timeout_ms=3000,
            )
            
            print("kafka ì—°ê²° ì„±ê³µ")
            connected = True
    
            # ì»¨ìŠˆë¨¸ ì—°ê²°ë˜ë©´ s3ë¡œ ì „ì†¡
            empty_count = 0  # ë©”ì‹œì§€ê°€ ì—†ì„ ë•Œ ì¹´ìš´íŠ¸í•  ë³€ìˆ˜
            while True:
                msg = consumer.poll(timeout_ms=1000)
                # ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ê¸°ë‹¤ë¦¼
                if msg is None:
                    empty_count += 1
                    print(f"{empty_count}: ë©”ì„¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    # 5ë²ˆ ì—°ì†ìœ¼ë¡œ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    if empty_count >= 3:
                        print("ë©”ì‹œì§€ê°€ 3ë²ˆ ì—°ì†ìœ¼ë¡œ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    continue

                else:
                    empty_count = 0  # ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
                    for message in msg.values():
                        for data in message:
                            try:
                                # Kafkaì—ì„œ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜¤ê¸°
                                data = data.value
                                print(f"Kafkaì—ì„œ ë°›ì€ ë°ì´í„°: {data}")

                                # Base64ë¡œ ì¸ì½”ë”©ëœ 'contents'ë¥¼ ë””ì½”ë”©í•˜ì—¬ ì›ë˜ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ë³µì›
                                decoded_content = base64.b64decode(data['contents'])

                                # ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ BytesIO ê°ì²´ë¡œ ë³µì›
                                file_obj = io.BytesIO(decoded_content)

                                # S3ë¡œ ì—…ë¡œë“œ
                                key = data['save_path']
                                bucket_name = 't1-tu-data'

                                hook = S3Hook(aws_conn_id='data')  # s3 ì—°ê²°
                                hook.get_conn().put_object(
                                Bucket=bucket_name,
                                Key=key,
                                Body=file_obj
                                )          
                                print(f"S3ì— ì—…ë¡œë“œ ì™„ë£Œ: {bucket_name}/{key}")
                            
                            except Exception as e:
                                print(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                                continue
                break
            consumer.close()
            print("task ì™„ë£Œ: consumerì™€ì˜ ì—°ê²°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.") 
        
        except Exception as e:
            print(f"kafka ì—°ê²° ì‹¤íŒ¨: {e}")
            attempt += 1
            print(f"{attempt}/{retry_count} ë²ˆì§¸ ì‹œë„ ì¤‘...")
            time.sleep(5)  # 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„


def success_noti():
    url = "https://notify-api.line.me/api/notify"
    data = {"message":"raw data ë³´ë‚´ê¸° ì™„ë£Œ ğŸ‘"}
    headers={"Authorization": 'Bearer UuAPZM7msPnFaJt5wXTUx34JqYKO7n3AUlLq4b3eyZ4'}
    response = requests.post(url, data, headers=headers)
    print("#"*35)
    print("airflow ì‘ì—…ì™„ë£Œ")
    print("#"*35)
    return True

def fail_noti():
    url = "https://notify-api.line.me/api/notify"
    data = {"message":"raw data ë³´ë‚´ê¸° ğŸ”¥ì‹¤íŒ¨ğŸ”¥"}
    headers={"Authorization": 'Bearer UuAPZM7msPnFaJt5wXTUx34JqYKO7n3AUlLq4b3eyZ4'}
    response = requests.post(url, data, headers=headers)
    print("#"*35)
    print("airflow ì‘ì—…ì‹¤íŒ¨")
    print("#"*35)
    return True

with DAG(
'kafka_to_S3',
default_args={
'depends_on_past': False,
'email_on_failure': False,
'email_on_retry': False,
'execution_timeout': timedelta(minutes=10),
'retries': 3,
'retry_delay':timedelta(minutes=3),
},
description='interpark DAG',
start_date=datetime(2024, 11, 21),
schedule_interval='@daily',
catchup=False,
tags=['interpark','kafka','s3']
) as dag:

    start = EmptyOperator(
            task_id='start'
            )

    end = EmptyOperator(
            task_id='end',
            trigger_rule ="one_success"
            )

    producer_to_kafka = PythonVirtualenvOperator(
            task_id='producer.to.kafka',
            python_callable=producer_to_kafka,
            requirements=[
                "git+https://github.com/hahahellooo/interpark.git@0.4/s3",
                ],
            system_site_packages=True
            )
    
    kafka_to_s3 = PythonOperator(
            task_id='kafka.to.s3',
            python_callable=kafka_to_s3
            )
    
    success_noti = PythonOperator(
            task_id='success.noti',
            python_callable=success_noti
            )

    fail_noti = PythonOperator(
            task_id='fail.noti',
            python_callable=fail_noti,
            trigger_rule="one_failed"
            )

    start >> producer_to_kafka >> kafka_to_s3
    kafka_to_s3 >> [success_noti, fail_noti] >> end
